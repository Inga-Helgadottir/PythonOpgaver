{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b52eee",
   "metadata": {},
   "source": [
    "# steps\n",
    "\n",
    "### Part 1\n",
    "1. Get data from: \"https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents \"\n",
    "2. Using BeatifullSoup get all the speeches from 1900-2022\n",
    "3. Load all speech urls into a dictionary with year as key Hint (get year with regex: r\"\\b(19|20)\\d{2}\\b\")\n",
    "4. Loop through dictionary and save content of each speech in [year].txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cf7611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75aec82a-101a-417b-b4f2-d2e2fecd4ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from: \"https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents\"\n",
    "url = \"https://en.wikisource.org/wiki/Portal:State_of_the_Union_Speeches_by_United_States_Presidents\"\n",
    "\n",
    "a = requests.get(url).text\n",
    "\n",
    "# Using BeatifullSoup get all the speeches from 1900-2022\n",
    "soup = BeautifulSoup(a, 'html.parser')\n",
    "\n",
    "links = {}\n",
    "\n",
    "for link in soup.find_all('a', href=True):  \n",
    "    if link['href'].endswith(\"_State_of_the_Union_Address\"):\n",
    "        nextSib = link.next_sibling.next_sibling.next_sibling\n",
    "        year = \"\";\n",
    "        startWikiLink = \"https://en.wikisource.org/\"\n",
    "        # Load all speech urls into a dictionary with year as key\n",
    "        if nextSib[-3:-1] == \" –\":\n",
    "            year = int(nextSib[-8:-4])\n",
    "            if year > 1899:\n",
    "                links.update({year: startWikiLink + nextSib.previous_sibling.previous_sibling.previous_sibling.get(\"href\")})\n",
    "        else:\n",
    "            year = int(nextSib[-5:-1])\n",
    "            if year > 1899:\n",
    "                links.update({year: startWikiLink + nextSib.previous_sibling.previous_sibling.previous_sibling.get(\"href\")})\n",
    "            \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfecdb35-c3a6-4c09-a4eb-24d3b1d22de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through dictionary and save content of each speech in [year].txt files\n",
    "for i in links:\n",
    "    yearFromLinks = str(i)\n",
    "    urlFromLinks = links.get(i)\n",
    "    fileName = \"speeches/\" + yearFromLinks + \".txt\"\n",
    "    with open(fileName, \"w\") as f:\n",
    "        f.write(urlFromLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b524bd-979b-4a05-85a3-6c301c72b10f",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "1. Install nltk: `pip install nltk`\n",
    "2. From the data/gdp.csv file create a dataframe with year and GDP\n",
    "3. From the data/US presidents.csv file create a dataframe with year, president and party\n",
    "4. From the developed text files in part 1, create a dictionary with year:speech\n",
    "5. Clean text by change all to lowercase and remove '\\n'\n",
    "6. Get words from texts (from nltk.tokenize import word_tokenize). Clean text by removing stop words (from nltk.corpus import stopwords) and all non-alphabetic characters (including , and .)\n",
    "7. Use from nltk.stem import WordNetLemmatizer to lemmatize all texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11f9d44b-4c25-4ff7-b50c-64a33e9a1b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.3.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "618422ca-2d5b-4bd6-b9b8-393a3d5d3391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    date  level-current\n",
      "0   1930           92.2\n",
      "1   1931           77.4\n",
      "2   1932           59.5\n",
      "3   1933           57.2\n",
      "4   1934           66.8\n",
      "..   ...            ...\n",
      "81  2011        15517.9\n",
      "82  2012        16155.3\n",
      "83  2013        16691.5\n",
      "84  2014        17427.6\n",
      "85  2015        18120.7\n",
      "\n",
      "[86 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# From the data/gdp.csv file create a dataframe with year and GDP\n",
    "df = pd.read_csv(\"../../data/gdp.csv\", usecols = [\"date\", \"level-current\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00a56059-6d7f-4f03-8631-e564fde848fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Years (after inauguration);President;Party\n",
      "0             1900;William McKinley;Republican\n",
      "1           1901;Theodore Roosevelt;Republican\n",
      "2           1902;Theodore Roosevelt;Republican\n",
      "3           1903;Theodore Roosevelt;Republican\n",
      "4           1904;Theodore Roosevelt;Republican\n",
      "..                                         ...\n",
      "116               2017;Donald Trump;Republican\n",
      "117               2018;Donald Trump;Republican\n",
      "118               2019;Donald Trump;Republican\n",
      "119               2020;Donald Trump;Republican\n",
      "120                    2021;Joe Biden;Democrat\n",
      "\n",
      "[121 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# From the data/US presidents.csv file create a dataframe with year, president and party\n",
    "df = pd.read_csv(\"../../data/US presidents.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a5683-bd75-4a60-a892-86eaf25fa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the developed text files in part 1, create a dictionary with year:speech\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9486ad-8244-47e2-9c0e-00d805760fba",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "**[TextBlob](https://textblob.readthedocs.io/en/dev/quickstart.html)** returns polarity and subjectivity of a sentence. Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment. Subjectivity quantifies the amount of personal opinion and factual information contained in the text. The higher subjectivity means that the text contains personal opinion rather than factual information\n",
    "1. Install both textblob for sentiment analysis and wordclouds (pip install textblob wordclouds) and download the vader lexicon (nltk.download('vader_lexicon'))\n",
    "2. Find the polarity and subjectivity of each text (Hint: `TextBlob(text).sentiment`)\n",
    "3. Is there a correlation between negativity and recession years?\n",
    "4. Create a word cloud for the cleaned up speeches of both Trump and Obama. What can be learned from the word clouds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a1de0a-78d0-4116-8699-abfcef8d9621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
